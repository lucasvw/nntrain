{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39365c74-e2c2-450e-a904-20121eb13c2f",
   "metadata": {},
   "source": [
    "# Acceleration\n",
    "\n",
    "> Module containing helper functions and classes around acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74887089-0d8d-451f-9afc-4e83f5d07983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e930880-05fb-4819-83f8-420a47467b16",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from operator import attrgetter\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "import math\n",
    "import torcheval.metrics as tem\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from nntrain.learner import Subscriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e36e5-c0d2-4e5d-bf9a-cf4f3d10e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class SGD:\n",
    "    def __init__(self, params, lr, wd=0.):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.i = 0\n",
    "\n",
    "    def step(self):                    # this is the method that get's called by the Learner\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                self.reg_step(p)       # first add regularization\n",
    "                self.opt_step(p)       # then do the actual step\n",
    "        self.i +=1\n",
    "\n",
    "    def opt_step(self, p):\n",
    "        p -= p.grad * self.lr          # regular step\n",
    "    \n",
    "    def reg_step(self, p):\n",
    "        if self.wd != 0:               # only regularize when the weight decay parameter is set\n",
    "            p *= 1 - self.lr*self.wd   # update the weights as described above\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b5c60-4077-4c86-b88a-3caa4b2fc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class Momentum(SGD):\n",
    "    def __init__(self, params, lr, wd=0., mom=0.9):\n",
    "        super().__init__(params, lr=lr, wd=wd)\n",
    "        self.mom=mom\n",
    "\n",
    "    def opt_step(self, p):\n",
    "        if not hasattr(p, 'grad_avg'): p.grad_avg = torch.zeros_like(p.grad)\n",
    "        p.grad_avg = p.grad_avg*self.mom + p.grad*(1-self.mom)\n",
    "        p -= self.lr * p.grad_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e86f2-3475-4d1e-926c-5bf541d9f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class RMSProp(SGD):\n",
    "    def __init__(self, params, lr, wd=0., sqr_mom=0.99, eps=1e-5):\n",
    "        super().__init__(params, lr=lr, wd=wd)\n",
    "        self.sqr_mom = sqr_mom\n",
    "        self.eps = eps\n",
    "\n",
    "    def opt_step(self, p):\n",
    "        if not hasattr(p, 'sqr_avg'): \n",
    "            p.sqr_avg = p.grad**2\n",
    "        p.sqr_avg = p.sqr_avg*self.sqr_mom + (1-self.sqr_mom)*p.grad**2\n",
    "        p -= self.lr * p.grad/(p.sqr_avg.sqrt() + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39bc2a4-1b8b-4522-8009-46c908954c41",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class Adam(SGD):\n",
    "    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n",
    "        super().__init__(params, lr=lr, wd=wd)\n",
    "        self.beta1,self.beta2,self.eps = beta1,beta2,eps\n",
    "\n",
    "    def opt_step(self, p):\n",
    "        if not hasattr(p, 'avg'): \n",
    "            p.avg = torch.zeros_like(p.grad.data)\n",
    "            p.sqr_avg = torch.zeros_like(p.grad.data)\n",
    "            \n",
    "        p.avg = self.beta1*p.avg + (1-self.beta1)*p.grad\n",
    "        unbias_avg = p.avg / (1 - (self.beta1**(self.i+1)))\n",
    "        p.sqr_avg = self.beta2*p.sqr_avg + (1-self.beta2)*(p.grad**2)\n",
    "        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i+1)))\n",
    "        p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a615d3-4ce8-444a-b3e1-509c28300f29",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class SchedulerS(Subscriber):\n",
    "    def __init__(self, scheduler_class):\n",
    "        self.scheduler_class = scheduler_class\n",
    "    \n",
    "    # intialize the scheduler instance after the optimizer has been intialized\n",
    "    def before_fit(self, learn):\n",
    "        self.scheduler = self.scheduler_class(learn.opt) \n",
    "        \n",
    "    # step the scheduler after the optimizer has stepped\n",
    "    def after_step(self, learn):\n",
    "        self.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59473a7a-424f-4c59-91db-c71a77ff8684",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "def conv_conn(in_c, out_c, kernel_size=3, stride=2):\n",
    "    return nn.Sequential(\n",
    "        conv_block(in_c, out_c, kernel_size=kernel_size, stride=1, act=True, norm=True),\n",
    "        conv_block(out_c, out_c, kernel_size=kernel_size, stride=stride, act=False, norm=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e78a5-0cec-4d97-95f7-c111a842533f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=2):\n",
    "        super().__init__()\n",
    "        self.in_c = in_c\n",
    "        self.out_c = out_c\n",
    "        self.stride = stride\n",
    "        self.conv_conn = conv_conn(in_c, out_c, stride=stride)\n",
    "        self.identity_conn = conv_block(in_c, out_c, kernel_size=1, stride=1, act=False, norm=False)\n",
    "        self.pooling = torch.nn.AvgPool2d(2, ceil_mode=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_conv = self.conv_conn(x)\n",
    "        if self.in_c == self.out_c: y_id = x\n",
    "        elif self.stride == 1:\n",
    "            y_id = self.identity_conn(x)\n",
    "        else:\n",
    "            y_id = self.pooling(self.identity_conn(x))\n",
    "        return self.relu(y_conv + y_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b658db8-081b-4452-a304-947adf2fe37e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "def resnet():\n",
    "    return nn.Sequential(                             # pixel grid input: 28x28  \n",
    "        ResBlock(1 , 8, stride=1),                    # 28x28\n",
    "        ResBlock(8 ,16),                              # 14x14\n",
    "        ResBlock(16,32),                              # 7x7\n",
    "        ResBlock(32,64),                              # 4x4\n",
    "        ResBlock(64,128),                             # 2x2\n",
    "        ResBlock(128,256),                            # 1x1\n",
    "        nn.Flatten(),                                 # flatten to 256 features\n",
    "        nn.Linear(256, 10, bias=False),               # linear layer to map to 10 output features\n",
    "        nn.BatchNorm1d(10)                            # final batchnorm layer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856e50b-9b61-4ecf-94ca-d4e30f28c906",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class ModelMonitorS(Subscriber):\n",
    "    \n",
    "    def __init__(self, modules): self.modules = modules\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        self.hooks = [Hook(i, module, partial(self.record_stats, learn)) for i, module in enumerate(self.modules)]\n",
    "        \n",
    "    def record_stats(self, learn, hook, layer, inp, outp):\n",
    "        if learn.model.training:\n",
    "            hook.nparams = sum(submodule.numel() for submodule in layer.parameters())\n",
    "            if isinstance(layer, ResBlock):\n",
    "                # K × K × Cin × Hout × Wout × Cout source=https://machinethink.net/blog/how-fast-is-my-model/\n",
    "                mac_conv1 = 9 * layer.in_c * inp[0].shape[2] * inp[0].shape[3] * layer.out_c\n",
    "                mac_conv2 = 9 * layer.out_c * outp.shape[2] * outp.shape[3] * layer.out_c    \n",
    "                hook.mac = (mac_conv1 + mac_conv2) / 1e6\n",
    "                if layer.stride != 1:\n",
    "                    # Add identity conv\n",
    "                    hook.mac += (layer.in_c * outp.shape[2] * outp.shape[3] * layer.out_c / 1e6)\n",
    "            else:\n",
    "                hook.mac = hook.nparams / 1e6\n",
    "            hook.batch_size = inp[0].shape[0]\n",
    "            hook.in_shape = list(inp[0].shape[1:])\n",
    "            hook.out_shape = list(outp.shape[1:])\n",
    "            \n",
    "    def after_batch(self, learn):\n",
    "        for h in self.hooks: h.remove()\n",
    "        raise CancelFitException                   # Only run this for a single batch, then cancel\n",
    "        \n",
    "    def __repr__(self):\n",
    "        out = f'{\"layer\":<20} : {\"input\":<20} : {\"output\":<20} : {\"# params\":>10} : {\"# MACs\":>10}\\n'\n",
    "        total_params = 0\n",
    "        total_mac = 0\n",
    "        for h in self.hooks:\n",
    "            out += f'{h.layer_name:<20} : {str(h.in_shape):<20} : {str(h.out_shape):<20} : {h.nparams:>10d} : {h.mac: 10.1f}\\n'\n",
    "            total_params += h.nparams\n",
    "            total_mac += h.mac\n",
    "        return f'{\"Total parameters:\":<20}{total_params:>10d} \\n{\"Total MACs:\":<20}{total_mac:10.1f} \\n\\n' + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f01d7-64ca-4cda-b3af-90d60bcd788d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class AugmentS(Subscriber):\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "    def before_batch(self, learn):\n",
    "        if learn.model.training:                    # augmentations are only applied to the training data\n",
    "            learn.batch[0] = self.transform(learn.batch[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
