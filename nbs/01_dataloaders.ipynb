{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a706372-b4f8-4d32-9470-24b837947fa5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"`nntrain`: Datasets and Dataloaders (1/n)\"\n",
    "author: \"Lucas van Walstijn\"\n",
    "date: \"2023-08-09\"\n",
    "categories: [code, neural network, deep learning]\n",
    "image: \"image_1.jpg\"\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: lucasvw/BlogComments\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a6911-402a-4234-9a58-7e1530bcb1bb",
   "metadata": {},
   "source": [
    "In this series, I want to discuss the creation of a small library for training neural networks: `nntrain`. It's based off the excellent [part 2](https://course.fast.ai/) of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the `miniai` library is discussed.\n",
    "\n",
    "The library will build upon PyTorch. However, we'll try as much as possible to build from scratch mainly to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch's version. This is similar to how things are done in the course. However, this is not just a \"copy / paste\" of the course: on many occasions I take a different route, and most of the code is my own. In my not so humble opinion the narrative presented here is slightly better ü§∑‚Äç‚ôÇÔ∏èüò±."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98b7c4-9002-407d-90f6-ea9d8c07b8ae",
   "metadata": {},
   "source": [
    "As we'll see, the library will be built using [`nb_dev`](https://nbdev.fast.ai/), another great project from the fastai community. With this software, it becomes very straight forward to create python libraries which are exported from jupyter notebooks. This may sound a bit weird, but it has the advantage that we can create the sourcecode for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure **while we are building the library**. For more details on why this is a good idea, see [here](https://www.fast.ai/posts/2022-07-28-nbdev2.html).\n",
    "\n",
    "So without further ado, let's start with where we left off in the previous [post](https://lucasvw.github.io/posts/08_nntrain_setup/):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de55ab2-bd58-40fe-8d16-7e9507741c5e",
   "metadata": {},
   "source": [
    "## End of last post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d50bf1-5741-4404-8217-0524df11e380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1cdbc1addf46208fc4c75590742e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0bc45e2b02428d97d8157a58855215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6708581e18044f4f8a43ad97d68a1223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56d9bd8032a48a1930b29d4c487f586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73d56073a6a4949a12855b8b9eb17f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/29.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47617d468b045b9b14b461fb253ff0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745e06016a944491ab47338b1273de7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cf98db1b0f4323a0b84f39b24d279c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n",
      "epoch=0 | loss=2.108 | acc=0.393\n",
      "epoch=1 | loss=1.854 | acc=0.503\n",
      "epoch=2 | loss=1.597 | acc=0.607\n",
      "epoch=3 | loss=1.390 | acc=0.622\n",
      "epoch=4 | loss=1.238 | acc=0.640\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,load_dataset_builder\n",
    "import torchvision.transforms.functional as TF   # to transform from PIL to tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "name = \"fashion_mnist\"\n",
    "ds_builder = load_dataset_builder(name)\n",
    "ds_hf = load_dataset(name, split='train')\n",
    "\n",
    "x_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\n",
    "y_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\n",
    "\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0,len(x_train), bs):\n",
    "            xb = x_train[i:i+bs]\n",
    "            yb = y_train[i:i+bs]\n",
    "\n",
    "            preds = model(xb)\n",
    "            acc = accuracy(preds, yb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n",
    "\n",
    "def accuracy(preds, targs):\n",
    "    return (preds.argmax(dim=1) == targs).float().mean()        \n",
    "\n",
    "def get_model_opt():\n",
    "    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    opt = torch.optim.SGD(model.parameters(), lr)\n",
    "    \n",
    "    return model, opt\n",
    "\n",
    "n_in  = 28*28\n",
    "n_h   = 50\n",
    "n_out = 10\n",
    "lr    = 0.01\n",
    "bs    = 1024\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36994e4-8b2a-41df-b785-aced0dc72a02",
   "metadata": {},
   "source": [
    "## Datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7367832-db8b-41ce-8f9a-7dff24078a45",
   "metadata": {},
   "source": [
    "For the next refactor, we want to tackle the minibatch construct we currently have in the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab697b3d-5ff6-4d70-bf05-bb266ad6b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# for i in range(0,len(x_train), bs):\n",
    "#     xb = x_train[i:i+bs]\n",
    "#     yb = y_train[i:i+bs]\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36309999-ce35-4c7d-bd23-2711f5099b1f",
   "metadata": {},
   "source": [
    "Instead of doing this manually, we would like to do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7595de2-6081-4121-baf5-8e237fac018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# for i in range(0,len(x_train), bs):\n",
    "#     xb, yb = dataset[i:i+bs]\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06715a-a1f3-4446-8712-ddf8a7286095",
   "metadata": {},
   "source": [
    "This is pretty straight-forward, we just need something we can index into and returns a tuple of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473d297-9350-4d05-93d8-e50ec88e09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x_train[i], self.y_train[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed559b97-3076-409f-bc59-65278531dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([784]), torch.Size([])]\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(x_train, y_train)\n",
    "print([i.shape for i in ds[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80573980-ba4e-49be-84af-0add348acc79",
   "metadata": {},
   "source": [
    "Because tensors behave very similar to numpy arrays, with this simple class we already have the behavior we need: i.e. slicing into the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab2872-ffc6-4e6e-883e-14372d0e58ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([9, 0, 0, 3, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7ee0c-c1d6-4da4-ade0-66506362d8e8",
   "metadata": {},
   "source": [
    "Next, we want to further improve the training loop and get to this behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b9b14-7115-4ae1-88ee-332425569603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# for xb, yb in dataloader:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d179a-c3df-44d4-9152-30a4ee8c2229",
   "metadata": {},
   "source": [
    "So our dataloader needs to wrap the dataset, and provide some kind of an iterator returning batches of data, based on the specified batch size. Let's create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fc527-eb1c-4db3-bfb0-d96d123d2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(0,len(self.dataset),self.batch_size):\n",
    "            yield self.dataset[i:i+self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bde37b-d6fb-42d8-8d88-42fb8af0e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb1da9-393c-46eb-83d1-a0bc794be0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in dl:\n",
    "            preds = model(xb)\n",
    "            acc = accuracy(preds, yb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa86f7-7415-48d1-886f-68c1d1155ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.072 | acc=0.326\n",
      "epoch=1 | loss=1.837 | acc=0.572\n",
      "epoch=2 | loss=1.580 | acc=0.635\n",
      "epoch=3 | loss=1.361 | acc=0.650\n",
      "epoch=4 | loss=1.205 | acc=0.656\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88be1a-d2e3-4984-a1bb-210139c30f5c",
   "metadata": {},
   "source": [
    "## Next up: shuffling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd88daf-5260-4c10-b40b-04d455197560",
   "metadata": {},
   "source": [
    "The above training loop already looks pretty good, it's small and concise, and fairly generic. The next improvement we are going to make is something that doesn't improve the code of the training loop, but improves training of the model. So far during training, we cycle each epoch through the data in the exact same order. This means that all training samples are always batched together with the exact same other samples. This is not good for training our model, instead we want to shuffle the data up. So that each epoch, we have batches of data that have not yet been batched up together. This additional variety helps the model to generalize as we will see.\n",
    "\n",
    "The simplest implementation would be to create a list of indices, which we put in between the dataset and the sampling of the mini-batches. In case we don't need to shuffle, this list will just be `[0, 1, ... len(dataset)]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2076aeb-868a-4c91-9fc0-aa2a9128f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, shuffle):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.indices = list(range(0, len(self.dataset)))\n",
    "        if self.shuffle: \n",
    "            random.shuffle(self.indices)\n",
    "            \n",
    "        for i in range(0,len(self.dataset),self.batch_size):\n",
    "            yield self.dataset[self.indices[i:i+self.batch_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe4320-5f5c-407d-be08-90fb5be3302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.099 | acc=0.400\n",
      "epoch=1 | loss=1.843 | acc=0.521\n",
      "epoch=2 | loss=1.583 | acc=0.625\n",
      "epoch=3 | loss=1.364 | acc=0.678\n",
      "epoch=4 | loss=1.249 | acc=0.638\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model_opt()\n",
    "dl = DataLoader(ds, bs, shuffle=True)\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e94cfd-24a2-4fc1-b0d2-818f2ebf08dd",
   "metadata": {},
   "source": [
    "This works just fine, but how can we encapsulate this logic in a separate class? Let's start with a simple `Sampler` class that we can iterate through and either gives indices in order, or shuffled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc5826-5927-47e1-9f74-8f5916fab1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, shuffle=False):\n",
    "        self.range = list(range(0, len(ds)))\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle: random.shuffle(self.range)\n",
    "        for i in self.range:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb1fda-fdb9-4efc-a310-9b5cda02f617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 2, 3, 4, 5, "
     ]
    }
   ],
   "source": [
    "s = Sampler(ds, False)           # shuffle = False\n",
    "for i, sample in enumerate(s): \n",
    "    print(sample, end=', ')\n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d0543-2d36-4816-8d55-78f20c39445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44880, 31024, 12590, 21343, 47390, 5890, "
     ]
    }
   ],
   "source": [
    "s = Sampler(ds, True)            # shuffle = TRUE\n",
    "for i, sample in enumerate(s): \n",
    "    print(sample, end=', ')\n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136eb35a-633c-4929-b72f-51a18329ac4a",
   "metadata": {},
   "source": [
    "Next, let's create a BatchSampler that does the same, but returns the indexes in batches. For that we can use the `islice()` function from the `itertools` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd3ef4-32ba-4357-82a4-fcee195f0c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def printlist(this): print(list(this))\n",
    "\n",
    "lst = list(range(0, 10))         # create a list of 10 numbers\n",
    "\n",
    "printlist(islice(lst, 0, 3))     # with islice we can get a slice out of the list\n",
    "printlist(islice(lst, 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0e2d3-77e0-412c-9aa1-346a92363332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "printlist(islice(lst, 4))        # we can also get the \"next\" 4 elements\n",
    "printlist(islice(lst, 4))        # doing that twice gives the same first 4 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47374626-6268-473d-b039-c786f8b46e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[4, 5, 6, 7]\n",
      "[8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "lst = iter(lst)                  # however if we put an iterator on the list:\n",
    "\n",
    "printlist(islice(lst, 4))        # first 4 elements\n",
    "printlist(islice(lst, 4))        # second 4 elements\n",
    "printlist(islice(lst, 4))        # remaining 2 elements\n",
    "printlist(islice(lst, 4))        # iterator has finished.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d61d2e-3df7-4b5b-ac17-5643176ca7b6",
   "metadata": {},
   "source": [
    "And thus we create our `BatchSampler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27912ceb-2693-444f-9eef-553e1edf4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler():\n",
    "    def __init__(self, sampler, batch_size):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        it = iter(self.sampler)\n",
    "        while True:\n",
    "            res = list(islice(it, self.batch_size))\n",
    "            if len(res) == 0:\n",
    "                return\n",
    "            yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014976ed-908d-4413-9688-2e193f41780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[4, 5, 6, 7]\n",
      "[8, 9]\n"
     ]
    }
   ],
   "source": [
    "s = Sampler(list(range(0,10)), shuffle=False)\n",
    "batchs = BatchSampler(s, 4)\n",
    "for i in batchs:\n",
    "    print(list(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27a07d-3e97-4e09-97ca-67ff668a647c",
   "metadata": {},
   "source": [
    "And let's incorporate it into the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40033fb3-35c4-4fda-bb68-699b58948b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_sampler):\n",
    "        self.dataset = dataset\n",
    "        self.batch_sampler = batch_sampler\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.batch_sampler:\n",
    "            yield self.dataset[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b2cd6-8e60-4abe-aa66-10cfd94375b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.134 | acc=0.367\n",
      "epoch=1 | loss=1.870 | acc=0.579\n",
      "epoch=2 | loss=1.594 | acc=0.632\n",
      "epoch=3 | loss=1.417 | acc=0.635\n",
      "epoch=4 | loss=1.279 | acc=0.646\n"
     ]
    }
   ],
   "source": [
    "s = Sampler(ds, shuffle=True)\n",
    "dl = DataLoader(ds, BatchSampler(s, bs))\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a3803-b07a-429b-9134-39b9947e62b5",
   "metadata": {},
   "source": [
    "And this works pretty good. However, there is one caveat. In the very beginning of this post we did:\n",
    "\n",
    "```\n",
    "x_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\n",
    "y_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\n",
    "```\n",
    "\n",
    "And that is something we ideally would also like to be part of the Dataloaders / Dataset paradigm. So instead of first transforming the Huggingface Dataset into `x_train` and `y_train`, we want to directly use the dataset. We can do so by adding a collate function. This wraps around a list of individual \"entries\" into the datasets, and receives a list of individual x,y tuples (`[(x_1,y_1), (x_2, y_2), ..]`) as argument. In that function, we can determine how to treat these items and parse it in a way that is suitable to our needs. i.e.:\n",
    "\n",
    "- batch the `x` and `y`, so that we transform from `[(x_1,y_1), (x_2, y_2), ..]`  to `[(x_1, x_2, ..), (y_1, y_2, y_3, ..)]`\n",
    "- move individual items `x_i` and `y_i` to tensors\n",
    "- stack the `x` tensors and `y` tensors respectively into one big tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e8af86-5c24-4db8-ba5e-48384c25cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_sampler, collate_func):\n",
    "        self.dataset = dataset\n",
    "        self.batch_sampler = batch_sampler\n",
    "        self.collate_func = collate_func\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.batch_sampler:\n",
    "            yield self.collate_func(self.dataset[i] for i in batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91359b28-5dc4-45cb-bf1d-c8daffb8ba7c",
   "metadata": {},
   "source": [
    "In this case in the `collate_func` we transform from PIL to tensor, get rid of the dictionary and zip up the results as is expected from the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabaedb9-77d8-4254-8d28-2c37483b38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(data):\n",
    "    data = [(TF.to_tensor(el['image']).view(-1), torch.tensor(el['label'])) for el in data]\n",
    "    x, y = zip(*data)\n",
    "    return torch.stack(x), torch.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86776f18-2e11-499f-8903-5ac15fdbc050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.045 | acc=0.359\n",
      "epoch=1 | loss=1.794 | acc=0.495\n",
      "epoch=2 | loss=1.588 | acc=0.569\n",
      "epoch=3 | loss=1.352 | acc=0.641\n",
      "epoch=4 | loss=1.242 | acc=0.646\n"
     ]
    }
   ],
   "source": [
    "s = Sampler(ds_hf, shuffle=True)\n",
    "dl = DataLoader(ds_hf, BatchSampler(s, bs), collate_func)\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0132c-bbd5-4d44-80c3-ece023e5196c",
   "metadata": {},
   "source": [
    "This is pretty nice, we have replicated the main logic of PyTorch's DataLoader. It has a slightly different API as we don't have to specify the `BatchSampler`, instead we can just pass `shuffle=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ba588-bb65-4969-827e-0e813d36a2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.121 | acc=0.363\n",
      "epoch=1 | loss=1.890 | acc=0.595\n",
      "epoch=2 | loss=1.660 | acc=0.605\n",
      "epoch=3 | loss=1.428 | acc=0.635\n",
      "epoch=4 | loss=1.284 | acc=0.630\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "s = Sampler(ds_hf, shuffle=True)\n",
    "dl = DataLoader(ds_hf, batch_size=bs, shuffle=True, collate_fn=collate_func)\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da52d55-2c52-4aa2-bd94-be854b29b42e",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2703d7-58b5-4ae8-b18c-e06b97c33a14",
   "metadata": {},
   "source": [
    "Let's add a validation set to make sure we validate on data we are not training on. For that we are going to pull the data from the datasets library without the `splits` argument, which will give us a dataset dictionary containing both a training and a test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a657046-2be7-4582-86c3-326714e26576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e074644ed16d4a3e9ebe2037d85cb1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_hf = load_dataset(name)\n",
    "ds_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9381f25-8248-4fc0-9ca1-d109d625e442",
   "metadata": {},
   "source": [
    "And let's create two dataloaders, one for the train and one for the validation set. For the validation loader we can double the batch size since we won't be computing gradients for the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570515a-278c-4b3e-9459-d895cec6f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(ds_hf['train'], batch_size=bs, shuffle=True, collate_fn=collate_func)\n",
    "valid_loader = DataLoader(ds_hf['test'], batch_size=2*bs, shuffle=False, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf65c44-5ed7-46d2-ba7d-f168c2b202c4",
   "metadata": {},
   "source": [
    "We change the training loop in a couple of ways:\n",
    "\n",
    "- compute loss and metrics more correctly, by taking care of the batch-size and taking the average over all data\n",
    "- add a seperate forward pass for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d188a9-6bfc-4c2d-a00b-7a5ebbe3089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                       # put the model in \"train\" mode\n",
    "        n_t = train_loss_s = 0                              # initialize variables for computing averages\n",
    "        for xb, yb in train_loader:\n",
    "            preds = model(xb)\n",
    "            train_loss = loss_func(preds, yb)\n",
    "            train_loss.backward()\n",
    "            \n",
    "            n_t += len(xb)\n",
    "            train_loss_s += train_loss.item() * len(xb)\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval()                                        # put the model in \"eval\" mode\n",
    "        n_v = valid_loss_s = acc_s = 0                      # initialize variables for computing averages\n",
    "        for xb, yb in valid_loader: \n",
    "            with torch.no_grad():                           # no need to compute gradients on validation set\n",
    "                preds = model(xb)\n",
    "                valid_loss = loss_func(preds, yb)\n",
    "                \n",
    "                n_v += len(xb)\n",
    "                valid_loss_s += valid_loss.item() * len(xb)\n",
    "                acc_s += accuracy(preds, yb) * len(xb)\n",
    "        \n",
    "        train_loss = train_loss_s / n_t                     # compute averages of loss and metrics\n",
    "        valid_loss = valid_loss_s / n_v\n",
    "        acc = acc_s / n_v\n",
    "        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c979153-56ee-451e-a53f-d2ffdc468645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | train_loss=2.185 | valid_loss=2.072 | acc=0.526\n",
      "epoch=1 | train_loss=1.952 | valid_loss=1.823 | acc=0.635\n",
      "epoch=2 | train_loss=1.685 | valid_loss=1.551 | acc=0.653\n",
      "epoch=3 | train_loss=1.430 | valid_loss=1.330 | acc=0.657\n",
      "epoch=4 | train_loss=1.243 | valid_loss=1.179 | acc=0.659\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model_opt()\n",
    "\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa467e-1e91-4cfa-8cc4-dae44647af65",
   "metadata": {},
   "source": [
    "And that's it for this post (almost)! We have seen a lot of details on Datasets, Dataloaders and the transformation of data. We have used these concepts to improve our training loop: shuffling the training data on each epoch, and the computation of the metrics on the validation set. But before we close off, let's make our very first exports into the library, so that next time we can continue where we finished off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd6117-f9c9-40fd-902e-3bd4201b35f7",
   "metadata": {},
   "source": [
    "## First exports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cfe11-c26d-4d2d-99de-0dc464c58895",
   "metadata": {},
   "source": [
    "When exporting code to a module with `nbdev` the first thing we need to do is declare the `default_exp` directive. This makes sure that when we run the export, the module will be exported to `dataloaders.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cf4a2-c9ca-452a-b378-69441144ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b19e3-fa3b-4549-ba3a-c159ff08e5d0",
   "metadata": {},
   "source": [
    "Next, we can export any code into the module by adding `#\\export` on top of the cell we want to export. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995453be-ca15-4f1a-8452-910132e5c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def print_hello():\n",
    "    print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf376b-09b9-48f8-8c72-dae27ef62978",
   "metadata": {},
   "source": [
    "To export, we simply execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bca1df-29a9-45fb-ab90-91c2b41d2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c85ff-13d0-49b0-ab9a-94bcf3532ebd",
   "metadata": {},
   "source": [
    "This will create a file called `dataloaders.py` in the library folder (in my case `nntrain`) with the contents:\n",
    "\n",
    "```\n",
    "# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_dataloaders.ipynb.\n",
    "\n",
    "# %% auto 0\n",
    "__all__ = ['func']\n",
    "\n",
    "# %% ../nbs/01_dataloaders.ipynb 59\n",
    "def print_hello():\n",
    "    print('hello')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c2ff0-5c48-4db1-9b95-e620845ef6af",
   "metadata": {},
   "source": [
    "So what do we want to export here? Let's see if we can create some generic code for loading data from the Huggingface datasets library into a PyTorch Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb3934-ae03-437a-93bb-89e33d4c2784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cb732-4797-4119-ad97-9ec0812aa843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def hf_ds_collate_func(data):\n",
    "    '''\n",
    "    Collation function for building a PyTorch DataLoader from a a huggingface dataset.\n",
    "    Tries to put all items from an entry into the dataset to tensor.\n",
    "    PIL images are converted to tensor.\n",
    "    '''\n",
    "\n",
    "    def to_tensor(i):\n",
    "        if isinstance(i, PIL.Image.Image):\n",
    "            return TF.to_tensor(i).view(-1)\n",
    "        else:\n",
    "            return torch.tensor(i)\n",
    "    \n",
    "    data = [map(to_tensor, el.values()) for el in data]  # map each item from a dataset entry through to_tensor()\n",
    "    data = zip(*data)                                    # zip data of any length not just (x,y) but also (x,y,z)\n",
    "    return (torch.stack(i) for i in data)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb8c8-7402-4c71-804d-ef4312336588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DataLoaders:\n",
    "    def __init__(self, train, valid):\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_dls(cls, train_ds, valid_ds, bs, collate_fn):\n",
    "        return (DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate_fn),\n",
    "                DataLoader(valid_ds, batch_size=bs*2, collate_fn=collate_fn))\n",
    "        \n",
    "    @classmethod\n",
    "    def from_hf_dd(cls, dd, batch_size):\n",
    "        return cls(*cls._get_dls(*dd.values(), batch_size, hf_ds_collate_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51006a-0f60-4b78-af1c-0421483dbdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                       \n",
    "        n_t = train_loss_s = 0                              \n",
    "        for xb, yb in dls.train:\n",
    "            preds = model(xb)\n",
    "            train_loss = loss_func(preds, yb)\n",
    "            train_loss.backward()\n",
    "            \n",
    "            n_t += len(xb)\n",
    "            train_loss_s += train_loss.item() * len(xb)\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval()                                        \n",
    "        n_v = valid_loss_s = acc_s = 0                      \n",
    "        for xb, yb in dls.valid: \n",
    "            with torch.no_grad():                           \n",
    "                preds = model(xb)\n",
    "                valid_loss = loss_func(preds, yb)\n",
    "                \n",
    "                n_v += len(xb)\n",
    "                valid_loss_s += valid_loss.item() * len(xb)\n",
    "                acc_s += accuracy(preds, yb) * len(xb)\n",
    "        \n",
    "        train_loss = train_loss_s / n_t                     \n",
    "        valid_loss = valid_loss_s / n_v\n",
    "        acc = acc_s / n_v\n",
    "        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c97699-a441-41ab-a146-8616ca2710ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_hf_dd(ds_hf, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128fd28-f5a0-482c-9b61-f9a043535b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | train_loss=2.175 | valid_loss=2.050 | acc=0.406\n",
      "epoch=1 | train_loss=1.917 | valid_loss=1.780 | acc=0.536\n",
      "epoch=2 | train_loss=1.651 | valid_loss=1.532 | acc=0.616\n",
      "epoch=3 | train_loss=1.427 | valid_loss=1.340 | acc=0.637\n",
      "epoch=4 | train_loss=1.262 | valid_loss=1.203 | acc=0.648\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model_opt()\n",
    "\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e47c4-2441-4081-84bb-14e680565c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de32fed-ca6d-4231-a657-5b45e56baff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
